{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXt6LU7yCthu"
      },
      "source": [
        "# Assignment 2 - RAG\n",
        "\n",
        "Parts which require your interaction are marked with `TODO:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3JwUEQnwCuh3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: sentence_transformers in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (4.1.0)\n",
            "Requirement already satisfied: tqdm in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (4.66.6)\n",
            "Requirement already satisfied: numpy in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (1.26.3)\n",
            "Requirement already satisfied: filelock in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (19.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.29.2)\n",
            "Requirement already satisfied: packaging in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sentence_transformers) (4.49.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sentence_transformers) (2.5.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sentence_transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: colorama in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
            "Requirement already satisfied: idna>=2.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: sympy==1.13.1 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the relevant dependencies\n",
        "!pip3 install datasets sentence_transformers tqdm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNC0Um-19gtf"
      },
      "source": [
        "Imagine your task is to build a question-answering (QA) system for a company. You are given a language model and have to create this product out of it.\n",
        "The requirements of the system need to adapt very quickly to the new data without training.\n",
        "For this, we will use **Retrieval Augmented Generation (RAG)**.\n",
        "The company insists you use their in-house LM model trained on multiple tasks, a _flan-t5-small_.\n",
        "You can test its QA functionality by asking the question _\"When ETH was founded?\"_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-N1qUaDZCthv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'1897'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example inference with the model.\n",
        "# TODO: run me to test the environment\n",
        "\n",
        "from transformers import pipeline\n",
        "vanilla_qa_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=\"cuda:0\", truncation=True)\n",
        "\n",
        "QUESTION = \"QUESTION: When was ETH founded?\"\n",
        "\n",
        "vanilla_qa_pipe(f\"{QUESTION} ANSWER:\", max_new_tokens=10)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SNIbWYrC9gtf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1854'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vanilla_qa_pipe(f\"\"\"\n",
        "        CONTEXT: ETH Zurich (German: Eidgenoessische Technische Hochschule Zurich; English:\n",
        "        Federal Institute of Technology Zurich) is a public research university in Zurich,\n",
        "        Switzerland. Founded in 1854 with the stated mission to educate engineers and scientists,\n",
        "        the university focuses primarily on science, technology, engineering, and mathematics. It\n",
        "        consistently ranks among the top universities in the world and its 16 departments span a\n",
        "        variety of disciplines and subjects.\n",
        "        {QUESTION}\n",
        "        ANSWER:\",\n",
        "    \"\"\",\n",
        "    max_new_tokens=10\n",
        ")[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7E7ivT39gtf"
      },
      "source": [
        "The first output is 1897, which is incorrect.\n",
        "\n",
        "This is not a problem, we can use RAG to automatically provide the passage from an [external source](https://en.wikipedia.org/wiki/ETH_Zurich) and make the model answer. Concatenating the first paragraph from Wikipedia to the question makes the model yield the correct answer 1854."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uvfbw-XVCthw"
      },
      "outputs": [],
      "source": [
        "# Define model function; do not modify\n",
        "from typing import List\n",
        "\n",
        "def rag_qa_pipe(question: str, passages: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Define the RAG pipeline which concatenates passages to the question.\n",
        "    :param question: Question text.\n",
        "    :param passages: Relevant text passages.\n",
        "    :return: Generated text from the pipeline.\n",
        "    \"\"\"\n",
        "    passages = \"\\n\".join([f\"CONTEXT: {c}\" for c in passages])\n",
        "    return vanilla_qa_pipe(f\"{passages}\\nQUESTION: {question}\\nANSWER: \", max_new_tokens=10)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKLM2jX59gtf"
      },
      "source": [
        "To make sure you understand the function `rag_qa_pipe`, ask some question without and with some relevant context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PJIxcohU9gtg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "September 27, 2017\n",
            "26 June 1997\n"
          ]
        }
      ],
      "source": [
        "# TODO: use rag_qa_pipeline some random question that you might have just to test this function\n",
        "\n",
        "print(rag_qa_pipe(\"When is the first book of Harry potter released?\", []))\n",
        "print(rag_qa_pipe(\"When is the first book of Harry potter released?\", [\"Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity and commercial success worldwide.\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMkyWZa9gtg"
      },
      "source": [
        "Start with the provided model and the first 500 questions from the validation part of the _SQuAD_ dataset. The dataset has a ground truth Wikipedia passage linked to it and you can directly use it.\n",
        "\n",
        "Then, compute the QA performance of the model with and without prepended passage using `rag_qa_pipe(question, passages)`.\n",
        "\n",
        "Report the average case-sensitive answer exact match (model output is identical to the gold answer, EM) and case-insensitive [answer F1 scores](https://kierszbaumsamuel.medium.com/f1-score-in-nlp-span-based-qa-task-5b115a5e7d41) (F1) for both setups.\n",
        "Because each question has multiple possible answers, take the maximum score for a model answer across all gold answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boC3-WoQCthx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [01:21<00:00,  6.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean exact match without passage: 0.020942408376963352\n",
            "Mean f1 score without passage: 0.07713648577522923\n",
            "Mean exact match with passage: 0.7085514834205934\n",
            "Mean f1 score with passage: 0.7728355607936759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# baseline model evaluation\n",
        "# TODO: the this cell requires <30 new lines\n",
        "\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"rajpurkar/squad\")\n",
        "\n",
        "def metric_exact_match(ans_pred: str, ans_true: str) -> float:\n",
        "    \"\"\"\n",
        "    Case-sensitive answer exact match, model output is identical to the gold answer.\n",
        "    :param ans_pred: Predicted answer\n",
        "    :param ans_true: Ground truth answer\n",
        "    :return: 1. if the answers are the same, 0. otherwise\n",
        "    \"\"\"\n",
        "    # TODO: ~1 line\n",
        "    return float(ans_pred == ans_true)\n",
        "\n",
        "def metric_f1(ans_pred: str, ans_true: str) -> float:\n",
        "    \"\"\"\n",
        "    Case-insensitive answer F1 score.\n",
        "    :param ans_pred: Predicted answer.\n",
        "    :param ans_true: Ground truth answer.\n",
        "    :return: F1 score between the predicted and ground truth answers.\n",
        "    \"\"\"\n",
        "    # TODO: ~10 lines\n",
        "    gt_toks = ans_true.split()\n",
        "    pred_toks = ans_pred.split()\n",
        "    comm = Counter(gt_toks) & Counter(pred_toks)\n",
        "    snum = sum(comm.values())\n",
        "    if snum == 0:\n",
        "        return 0.0\n",
        "    precision = snum / len(pred_toks)\n",
        "    recall = snum / len(gt_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "    # hint: use `line[\"question\"]`, `line[\"context\"]`, and `line[\"answers\"]`\n",
        "    # TODO: run with and without prepended passage\n",
        "    # Run the model without prepended passage\n",
        "    pred_without_passage = vanilla_qa_pipe(f\"QUESTION: {line['question']} ANSWER:\", max_new_tokens=10)[0][\"generated_text\"]\n",
        "\n",
        "    # Run the model with prepended passage\n",
        "    pred_with_passage = vanilla_qa_pipe(f\"CONTEXT: {line['context']}\\nQUESTION: {line['question']}\\nANSWER:\", max_new_tokens=10)[0][\"generated_text\"]\n",
        "\n",
        "    # Compute exact match and F1 scores\n",
        "    em_without_passage = max(metric_exact_match(pred_without_passage, ans) for ans in line[\"answers\"][\"text\"])\n",
        "    f1_without_passage = max(metric_f1(pred_without_passage, ans) for ans in line[\"answers\"][\"text\"])\n",
        "\n",
        "    em_with_passage = max(metric_exact_match(pred_with_passage, ans) for ans in line[\"answers\"][\"text\"])\n",
        "    f1_with_passage = max(metric_f1(pred_with_passage, ans) for ans in line[\"answers\"][\"text\"])\n",
        "\n",
        "    # Store the results\n",
        "    if \"results\" not in locals(): \n",
        "        results = {\"em_without_passage\": [], \"f1_without_passage\": [], \"em_with_passage\": [], \"f1_with_passage\": []}\n",
        "    results[\"em_without_passage\"].append(em_without_passage)\n",
        "    results[\"f1_without_passage\"].append(f1_without_passage)\n",
        "    results[\"em_with_passage\"].append(em_with_passage)\n",
        "    results[\"f1_with_passage\"].append(f1_with_passage)\n",
        "\n",
        "# TODO: Print mean of the exact match and mean of F1 scores for the model with and without prepended passage\n",
        "print(\"Mean exact match without passage:\", np.mean(results[\"em_without_passage\"]))\n",
        "print(\"Mean f1 score without passage:\", np.mean(results[\"f1_without_passage\"]))\n",
        "print(\"Mean exact match with passage:\", np.mean(results[\"em_with_passage\"]))\n",
        "print(\"Mean f1 score with passage:\", np.mean(results[\"f1_with_passage\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x562pF1X9gtg"
      },
      "source": [
        "You will likely see improvements in scores by providing a passage to the model.\n",
        "\n",
        "In contrast to the previous evaluation, during inference in a real world scenario, we do not have access to the ground truth passage.\n",
        "All we have access to is the question from a user.\n",
        "Luckily, the company is providing you with an unstructured knowledge base. This could be the whole of Wikipedia but in our scenario, we use all the passages from the SQuAD dataset and shuffle them to remove any existing structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL7N2sKD9gtg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2067 passages in the knowledge base\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "kb = list(set(dataset[\"validation\"][\"context\"]))\n",
        "\n",
        "# make sure that there is no remaining structure\n",
        "random.Random(42).shuffle(kb)\n",
        "print(len(kb), \"passages in the knowledge base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWQi9Z2C9gtg"
      },
      "source": [
        "Now whenever we receive a question, we need to find the relevant passage(s) from the knowledge base and put it in the model input.\n",
        "This is a non-trivial task and a whole research field of Information Retrieval is devoted to it.\n",
        "\n",
        "\n",
        "We are going to convert all the knowledge base passages into vectors using TF-IDF and the provided embedding model ([bert-base-nli-max-tokens](https://huggingface.co/sentence-transformers/bert-base-nli-max-tokens)).\n",
        "The model inference is already implemented for you but you need to fill in all the functions in the `KnowledgeBase` class.\n",
        "You will need to implement the retrieval, the distance metrics, and the three similarity metrics (Euclidean, cosine, inner product).\n",
        "\n",
        "We need to build an abstraction for the knowledge base. It needs to support:\n",
        "- adding new keys (vectors) and their corresponding values\n",
        "- retrieving the closest key given one, based on 3 vector distance metrics\n",
        "\n",
        "The implementation does not need to be efficient.\n",
        "\n",
        "Hint: it's ok to just add all the elements to a list and on retrieval sort the list by the distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MFKeERqlCthx"
      },
      "outputs": [],
      "source": [
        "# Knowledge base building. This cell requires <20 new lines.\n",
        "from typing import Literal, List, Any\n",
        "\n",
        "Vec = List\n",
        "Val = Any\n",
        "\n",
        "class KnowledgeBase:\n",
        "    def __init__(self, dim: int):\n",
        "        \"\"\"\n",
        "        Initialize a knowledge base with a given dimensionality.\n",
        "        :param dim: the dimensionality of the vectors to be stored\n",
        "        \"\"\"\n",
        "        # TODO: initialize a persistent structure, such as a simple list\n",
        "        self.data = []\n",
        "        self.dim = dim\n",
        "\n",
        "    def add_item(self, key: Vec, val: Val):\n",
        "        \"\"\"\n",
        "        Store the key-value pair in the knowledge base.\n",
        "        :param key: key\n",
        "        :param val: value\n",
        "        \"\"\"\n",
        "        # TODO: add to the persistent structure\n",
        "        self.data.append((key, val))\n",
        "        \n",
        "    def retrieve(\n",
        "        self, key: Vec, metric: Literal['l2', 'cos', 'ip'], k: int = 1\n",
        "    ) -> List[Val]:\n",
        "        \"\"\"\n",
        "        Retrieve the top k values from the knowledge base given a key and similarity metric.\n",
        "        :param key: key\n",
        "        :param metric: Similarity metric to use.\n",
        "        :param k: Top k similar items to retrieve.\n",
        "        :return: List of top k similar values.\n",
        "        \"\"\"\n",
        "        # TODO: retrieve the k closest vectors and return their corresponding values\n",
        "        # Hint: this does not have to be efficient, feel free to just sort the whole persistent structure and return the top k\n",
        "        if metric == 'l2':\n",
        "            similarity_func = self._sim_euclidean\n",
        "        elif metric == 'cos':\n",
        "            similarity_func = self._sim_cosine\n",
        "        elif metric == 'ip':\n",
        "            similarity_func = self._sim_inner_product\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "        # Compute similarity for all items and sort by similarity\n",
        "        similarities = []\n",
        "        for stored_key, stored_val in self.data:\n",
        "            similarity = similarity_func(key, stored_key)\n",
        "            similarities.append((similarity, stored_val))\n",
        "        sorted_items = sorted(similarities, key=lambda x: x[0]) if metric == 'l2' else sorted(similarities, key=lambda x: x[0], reverse=True)\n",
        "        # Return the top k values\n",
        "        return [val for _, val in sorted_items[:k]]\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_euclidean(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute Euclidean (L2) distance between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the Euclidean distance between two vectors\n",
        "        a, b = np.array(a), np.array(b)\n",
        "        distance = np.sqrt(np.sum((a - b) ** 2))\n",
        "        return distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_cosine(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute the cosine similarity between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the cosine distance between two vectors\n",
        "        a, b = np.array(a), np.array(b)\n",
        "        norm_a = np.linalg.norm(a)\n",
        "        norm_b = np.linalg.norm(b)\n",
        "        if norm_a == 0 or norm_b == 0:\n",
        "            return 0.0\n",
        "        similarity = np.dot(a, b) / (norm_a * norm_b)\n",
        "        return similarity\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_inner_product(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute the inner product between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the iner product distance between two vectors\n",
        "        a, b = np.array(a), np.array(b)\n",
        "        similarity = np.dot(a, b)\n",
        "        return similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AUv2PWPqCthx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2067/2067 [00:27<00:00, 73.98it/s]\n"
          ]
        }
      ],
      "source": [
        "# Build knowledge base index\n",
        "# In ideal case this does not need to be changed and can just be run.\n",
        "# Make modifications if you feel they are necessary.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # Sparse retrieval using TF-IDF - vectorize with tfidf and retrieve\n",
        "vectorizer = TfidfVectorizer(max_features=768, norm=None)\n",
        "kb_vectorized = np.asarray(vectorizer.fit_transform([x for x in kb]).todense())\n",
        "kb_index_tfidf = KnowledgeBase(dim=768)\n",
        "for passage_index, passage_embd in enumerate(kb_vectorized):\n",
        "    kb_index_tfidf.add_item(passage_embd.squeeze(), passage_index)\n",
        "\n",
        "# Dense retrieval using Sentence Transformers\n",
        "model_embd = SentenceTransformer(\"bert-base-nli-mean-tokens\").to(\"cuda:0\")\n",
        "kb_index_embd = KnowledgeBase(dim=768)\n",
        "for passage_index, passage_embd in enumerate(tqdm.tqdm(kb)):\n",
        "    kb_index_embd.add_item(model_embd.encode(passage_embd).squeeze(), passage_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyVH1Zr9gtg"
      },
      "source": [
        "For the same first 500 questions from the validation split evaluate how often is the retrieved passage the correct one (formally Recall@1) or among the top 5 retrieved (Recall@5).\n",
        "Perform the retrieval with three distance metrics: euclidean distance, cosine distance, and inner product. The result for this should be 12 numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzfSY98f9gtg"
      },
      "source": [
        "In production, you receive a question from the user and to answer it, you need to first retrieve the relevant passage(s), pass it to the model, and only then generate the answer.\n",
        "\n",
        "Evaluate the model performance with passages retrieved by TFIDF and EMBD vectorization.\n",
        "Consider top-1 and top-5 passages.\n",
        "This time use only case-insensitive F1.\n",
        "The result for this cell should be |vectorizations $\\times$ passage sizes $\\times$ distance metrics = 2 x 2 x 3 = 12 numbers.\n",
        "\n",
        "Answer the following questions:\n",
        "* Provide one potential advantage and two potential disadvantages of using multiple retrieved passages?\n",
        "* Describe one approach to detect if none of the retrieved passages is relevant to the user question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oazFAFIBP2u2"
      },
      "source": [
        "TODO:\n",
        "### Answer to question1\n",
        "* Advantage: improving the coverage of answers: using multiple passages increases the likelihood that at least one passage contains the correct answer, this is especially useful when the top-1 passage might be incorrect due to noise or ambiguity. Disadvantages: increasing computational cost; increasing the risk of introducing irrelevant or misleading information.\n",
        "### Answer to question2\n",
        "* Assign a relevance score to each retrieved passage based on its similarity to the question (e.g., cosine similarity for embeddings). Set a predefined threshold for relevance (e.g., based on validation data where passages below a certain similarity score are unlikely to contain the answer). If none of the retrieved passages exceed this threshold, flag them as irrelevant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LF9KJ8dXCthx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:28<00:00, 17.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@1: 0.234\n",
            "Recall@5: 0.522\n",
            "COS\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:35<00:00, 14.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@1: 0.238\n",
            "Recall@5: 0.556\n",
            "IP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:18<00:00, 27.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@1: 0.21\n",
            "Recall@5: 0.554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# In ideal case this does not need to be changed and can just be run.\n",
        "# Make modifications if you feel they are necessary.\n",
        "\n",
        "retrieval_results = {\"recall_at_1\": {}, \"recall_at_5\": {}}\n",
        "for metric in [\"l2\", \"cos\", \"ip\"]:\n",
        "    print(metric.upper())\n",
        "    retrieval_results[\"recall_at_1\"][metric] = []\n",
        "    retrieval_results[\"recall_at_5\"][metric] = []\n",
        "    \n",
        "    for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "        # TODO: evaluate the retrieval\n",
        "        # TODO: store RAG model output\n",
        "        # This requires <30 new lines\n",
        "        # Retrieve top-1 and top-5 passages using the current metric\n",
        "        top_1_passages = kb_index_embd.retrieve(model_embd.encode(line[\"question\"]).squeeze(), metric=metric, k=1)\n",
        "        top_5_passages = kb_index_embd.retrieve(model_embd.encode(line[\"question\"]).squeeze(), metric=metric, k=5)\n",
        "\n",
        "        # Check if the correct passage is in the retrieved passages\n",
        "        correct_passage = line[\"context\"]\n",
        "        recall_at_1 = int(correct_passage in [kb[idx] for idx in top_1_passages])\n",
        "        recall_at_5 = int(correct_passage in [kb[idx] for idx in top_5_passages])\n",
        "\n",
        "        # Store the results\n",
        "        retrieval_results[\"recall_at_1\"][metric].append(recall_at_1)\n",
        "        retrieval_results[\"recall_at_5\"][metric].append(recall_at_5)\n",
        "\n",
        "    # Print the average recall scores for the current metric\n",
        "    print(f\"Recall@1: {np.mean(retrieval_results['recall_at_1'][metric])}\")\n",
        "    print(f\"Recall@5: {np.mean(retrieval_results['recall_at_5'][metric])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rokgCOi9gtg"
      },
      "source": [
        "Answer the following questions about similarity metrics:\n",
        "* Compare and contrast the three metrics, what they might be influenced by, and their advantages and disadvantages.\n",
        "* Consider the scenario if the vectors in the knowledge base were normalized so that $|x|_2 = 1$. What would the results look like? Hint: look at the formulas with this vector assumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oFY9cn_Qhvr"
      },
      "source": [
        "TODO:\n",
        "### Answer to question1\n",
        "* Euclidean distance: measures straight-line distance, sensitive to magnitude and dimensionality. Influenced by vector magnitude, noise, high-dimensional effects. Advantage: effetive when magnitude matters. Disadvantages: poor in high dimensions, sensitive to magnitude, more computational cost.\n",
        "* Cosine similarity: measures angular distance, ignores magnitude. Influenced by vector direction, normalization precision and sparsity. Advantages: robust for embeddings, effective for semantic similarity. Disadvantages: more computational cost, less effective for sparse vectors with low overlap.\n",
        "* Inner product: measures dot product, sensitive to both magnitude and direction. Influenced by vector scale, magnitude and sparsity. Advantages: computationally efficient, effective with normalization. Disadvantages: Performance depends on how the embedding model scales vectors; inconsistent scaling can reduce effectiveness.\n",
        "### Answer to question2 \n",
        "* When the vectors in the knowledge base were normalized so that $|x|_2 = 1$, all metrics will be equivalent in ranking, inner product metric will be the most efficient one. The result like retrieval performance (e.g.,Recall@1 and Recall@5) will be identical across metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBtKQELq9gtg"
      },
      "source": [
        "Lastly, it is a good practice to analyze failure cases of your solution to better understand the pipeline.\n",
        "Find the first example of each and compute how often the situation happens (percentage). Use the maximum exact match to determine correctness and L2 + embedding for retrieval.\n",
        "\n",
        "- For top-1: The retrieved passage is **correct** but the model is **not correct**.\n",
        "- For top-1: The retrieved passage is **not correct** but the model is **still correct**.\n",
        "- For top-5: One of the retrieved passages is the **correct** one but the model is **not correct**.\n",
        "- For top-1: Without retrieved passage is the model **correct** but with the passage the model becomes **incorrect**.\n",
        "- For top-1: Without retrieved passage is the model **incorrect** and with the passage the model becomes **incorrect** but in a different way (different answer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SX0zSnot9gth"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phenomenon 1 frequency: 5.6000000000000005 %\n",
            "Phenomenon 2 frequency: 4.0 %\n",
            "Phenomenon 3 frequency: 32.800000000000004 %\n",
            "Phenomenon 4 frequency: 0.6 %\n",
            "Phenomenon 5 frequency: 68.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# compute the 5 phenomena statistics (relative frequency) and find examples\n",
        "\n",
        "# TODO: <30 lines\n",
        "for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "  # Retrieve top-1 passage using L2 embedding\n",
        "  top_1_passage = kb_index_embd.retrieve(model_embd.encode(line[\"question\"]).squeeze(), metric=\"l2\", k=1)[0]\n",
        "  retrieved_passage = kb[top_1_passage]\n",
        "\n",
        "  # Check the phenomena\n",
        "  correct_passage = line[\"context\"]\n",
        "  pred_with_passage = rag_qa_pipe(line[\"question\"], [retrieved_passage])\n",
        "  pred_without_passage = rag_qa_pipe(line[\"question\"], [])\n",
        "\n",
        "  em_with_passage = max(metric_exact_match(pred_with_passage, ans) for ans in line[\"answers\"][\"text\"])\n",
        "  em_without_passage = max(metric_exact_match(pred_without_passage, ans) for ans in line[\"answers\"][\"text\"])\n",
        "\n",
        "  # Phenomenon 1: Retrieved passage is correct but model is not correct\n",
        "  if retrieved_passage == correct_passage and em_with_passage == 0:\n",
        "    if \"phenomenon_1\" not in locals():\n",
        "      phenomenon_1 = []\n",
        "    phenomenon_1.append(line)\n",
        "\n",
        "  # Phenomenon 2: Retrieved passage is not correct but model is still correct\n",
        "  if retrieved_passage != correct_passage and em_with_passage == 1:\n",
        "    if \"phenomenon_2\" not in locals():\n",
        "      phenomenon_2 = []\n",
        "    phenomenon_2.append(line)\n",
        "\n",
        "  # Phenomenon 3: One of the top-5 retrieved passages is correct but model is not correct\n",
        "  top_5_passages = kb_index_embd.retrieve(model_embd.encode(line[\"question\"]).squeeze(), metric=\"l2\", k=5)\n",
        "  top_5_retrieved = [kb[idx] for idx in top_5_passages]\n",
        "  if correct_passage in top_5_retrieved and em_with_passage == 0:\n",
        "    if \"phenomenon_3\" not in locals():\n",
        "      phenomenon_3 = []\n",
        "    phenomenon_3.append(line)\n",
        "\n",
        "  # Phenomenon 4: Without retrieved passage model is correct but with passage it becomes incorrect\n",
        "  if em_without_passage == 1 and em_with_passage == 0:\n",
        "    if \"phenomenon_4\" not in locals():\n",
        "      phenomenon_4 = []\n",
        "    phenomenon_4.append(line)\n",
        "\n",
        "  # Phenomenon 5: Without retrieved passage model is incorrect and with passage it becomes incorrect in a different way\n",
        "  if em_without_passage == 0 and em_with_passage == 0 and pred_with_passage != pred_without_passage:\n",
        "    if \"phenomenon_5\" not in locals():\n",
        "      phenomenon_5 = []\n",
        "    phenomenon_5.append(line)\n",
        "\n",
        "# Compute relative frequencies\n",
        "total_examples = len(dataset[\"validation\"].select(range(500)))\n",
        "print(\"Phenomenon 1 frequency:\", len(phenomenon_1) / total_examples * 100, \"%\")\n",
        "print(\"Phenomenon 2 frequency:\", len(phenomenon_2) / total_examples * 100, \"%\")\n",
        "print(\"Phenomenon 3 frequency:\", len(phenomenon_3) / total_examples * 100, \"%\")\n",
        "print(\"Phenomenon 4 frequency:\", len(phenomenon_4) / total_examples * 100, \"%\")\n",
        "print(\"Phenomenon 5 frequency:\", len(phenomenon_5) / total_examples * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fo6hkfq9gti"
      },
      "source": [
        "A client is complaining that the model answers incorrectly the question _\"Who is the current Governor of Victoria?\"_.\n",
        "1. Show your model output to this question with top-1 retrieved passage using any metric.\n",
        "2. Show which top-1 context is retrieved by L2 embd.\n",
        "\n",
        "Hint for the correct answer, see: [en.wikipedia.org/wiki/Premier_of_Victoria](https://en.wikipedia.org/wiki/Premier_of_Victoria)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aTa9yP289gti"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved passage by using metric cosine similarity: The Premier of Victoria is the leader of the political party or coalition with the most seats in the Legislative Assembly. The Premier is the public face of government and, with cabinet, sets the legislative and political agenda. Cabinet consists of representatives elected to either house of parliament. It is responsible for managing areas of government that are not exclusively the Commonwealth's, by the Australian Constitution, such as education, health and law enforcement. The current Premier of Victoria is Daniel Andrews.\n",
            "Model's answer: Daniel Andrews\n",
            "Top-1 context retrieved by l2 embedding: The Premier of Victoria is the leader of the political party or coalition with the most seats in the Legislative Assembly. The Premier is the public face of government and, with cabinet, sets the legislative and political agenda. Cabinet consists of representatives elected to either house of parliament. It is responsible for managing areas of government that are not exclusively the Commonwealth's, by the Australian Constitution, such as education, health and law enforcement. The current Premier of Victoria is Daniel Andrews.\n"
          ]
        }
      ],
      "source": [
        "QUESTION = \"Who is the premier of Victoria?\"\n",
        "# TODO: < 20 lines\n",
        "# Retrieve the top-1 passage using the specified metric cosine similarity\n",
        "top_1_passage = kb_index_embd.retrieve(model_embd.encode(QUESTION).squeeze(), metric=\"cos\", k=1)[0]\n",
        "retrieved_passage = kb[top_1_passage]\n",
        "# Generate the model's answer using the retrieved passage\n",
        "answer = rag_qa_pipe(QUESTION, [retrieved_passage])\n",
        "# Print the retrieved passage and the model's answer\n",
        "print(\"Retrieved passage by using metric cosine similarity:\", retrieved_passage)\n",
        "print(\"Model's answer:\", answer)\n",
        "\n",
        "# Retrieve the top-1 context using 2 embedding\n",
        "top_1_context_l2 = kb_index_embd.retrieve(model_embd.encode(QUESTION).squeeze(), metric=\"l2\", k=1)[0]\n",
        "retrieved_context_l2 = kb[top_1_context_l2]\n",
        "\n",
        "# Print the retrieved context\n",
        "print(\"Top-1 context retrieved by l2 embedding:\", retrieved_context_l2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBnDF4nTOe7"
      },
      "source": [
        "Answer the following questions:\n",
        "* Provide a reason why your model is giving the incorrect answer. (information tracing)\n",
        "* Propose a way by which this could be remedied. (information editing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddscx0DUTTiX"
      },
      "source": [
        "TODO:\n",
        "### Answer to question1\n",
        "* The model is giving the incorrect answer because the retrieved passage contains outdated information. The knowledge base used by the RAG system has not been updated with the latest information about the Premier of Victoria. The model is accurately answering the question based on the information it has, but that information is no longer correct.\n",
        "\n",
        "### Answer to question2\n",
        "* In order to remedy this issue, the first intuitive way will be updating the knowledge base, regularly update the knowledge base with the latest information from reliable sources. This could involve scraping Wikipedia pages, using news APIs, or other methods to ensure the data is current."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KcDRiRl9gti"
      },
      "source": [
        "Note on compute: the GPU time of the gold solution is ~15 minutes. If your solution requires much more compute (e.g. hours), then you are likely doing something incorrectly."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PyTorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
